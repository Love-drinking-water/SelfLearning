[TOC]

# 决策树

决策树是一种基本的分类与回归的方法，它可以认为是定义在特征空间与类空间上的**条件概率分布**，主要优点是模型具有可读性，分类速度快。

决策树的代表算法，主要是Quinlan在1986年提出ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。下面主要回顾这三种算法。

## 1. 决策树模型与学习

可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程：有决策树的根结点到叶结点的每一条路径构建一条规则；路径的内部结点的特征对应着规则的条件，而叶结点的类别对应着规则的结论。

决策树学习的策略是以损失函数为目标函数的最小化，当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题，因为从所有可能的决策树中选择最优决策树是**NP完全问题**，所以现实中决策树的学习算法通常采用启发式方法，近似求解这一最优问题，这样得到决策树是次最优（sub-optimal）的。

决策树的学习通常包含三个步骤，**特征选择**，**决策树生成**和**决策树的修剪**。

### 1.1 特征选择

特征选择，即从所有的特征中，选择一个“最优”的特征，放于决策树的结点中。这个“最优”的定义，主要是特征对于预测标签的预测能力或者区分能力。

假设某个特征为$A$,标签为$D$，那么这个预测能力可以用函数$f(A,D)$来表示，即当选择特征$A$时，特征$A$对标签的预测能力或者是区分能力。当标签$D$是类别变量时（分类任务），函数$f$往往是**信息增益**、**信息增益比**和**基尼增益**；而当标签$D$是连续变量时（回归任务），函数$f$往往采用**均方误差**。

### 1.2 决策树的生成

在第一步特征选择完成后，接下来需要确定该特征的划分，即找该特征的分割点，进行树的生长。

### 1.3 决策树的修剪

在以上两步完成后，这样的决策树往往对训练数据的分类很准确，但对未知数据集的预测能力有限，即出现过拟合现象。此时需要对构建的决策树进行剪枝（简化决策树）。

决策树的剪枝往往通过极小化决策树整体的损失函数或者代价函数来实现。通常损失函数往往定义为$C_{\alpha}(T)=C(T)+\alpha|T|$，其中$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂程度（叶子结点的个数），参数$\alpha \geq 0$控制两者之间的影响。较大的$\alpha$促使选择较简单的树，较小的$\alpha$促使选择复杂的树。$\alpha=0$表示不考虑模型的复杂度，只考虑模型的拟合程度。

剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。当$\alpha$确定时，子树越大，往往模型的拟合程度越高，但是模型的复杂程度越高；相反，子树越小，模型的复杂程度越低，但是此时模型的拟合程度不好，**损失函数恰好表示了两者的平衡**。

之所以决策树存在过拟合情况，就是在建树时，往往只考虑对训练数据更好的拟合，没有考虑减小模型的复杂程度，而决策树剪枝则通过优化损失函数，在保证一定数据拟合程度的基础上，降低模型复杂度，增加模型的泛化能力。可以说**决策树的生成是学习局部的模型，而决策树的剪枝是学习整体的模型**。

剪枝往往有两种策略，预剪枝和后剪枝。

- **预剪枝**

  在决策树的生成过程中，每增加一个结点，均考虑该结点增加前后的损失有无减小，如果损失增大，显然不适合继续生长。

- **后剪枝**

  在决策树的建树过程中不做干扰，在建树完成后，自底向上进行剪枝，如果去掉该结点后损失减小那么就将该结点去掉，反之则保留。

## 2. ID3算法

ID3算法主要解决的是分类任务，且特征限定为类别类型。

### 2.1 特征选择

在特征选择中，使用**信息增益**来选择特征（使用信息增益来衡量特征对标签的预测能力），假设特征为$A$，标签为$D$，则特征$A$对$D$的预测能力$f(A,D)$定义如下，

$$
f(A,D)=H(D)-H(D|A)
$$

> 其中$H(D)$表示变量$D$的信息熵，$H(D)=-\sum_{i=1}^{d}p_i \log p_i$，$d$表示变量$D$中类别的个数，$H(D|A)$表示基于特征$A$的条件熵，$H(D|A)=\sum_{i=1}^{a}{\frac{|D_i|}{|D|}H(D_i)}$,$a$表示变量$A$中特征的类别个数

在函数$f(A,D)$确定后，只需遍历所有特征，找到函数值（信息增益）最大的特征，即完成了特征选择过程。

### 2.2 决策树的生成

因为ID3算法限定数据类型为类别，所以在特征选择步骤完成后，只需要按照特征类别进行分支即可（特征有几个类别则分几个支），然后迭代第一步和第二步即可完成决策树的创建。

> 原生的ID3算法中没有剪枝这一步，所以ID3算法容易过拟合

## 3. C4.5算法

C4.5算法主要解决的是分类任务，特征可以数值类型，也可以是连续类型，且特征中允许存在缺失值。

### 3.1 特征选择

在特征选择中，使用**信息增益比**来选择特征，假定特征为$A$，标签为$D$，则特征$A$对$D$的预测能力$f(A,D)$定义如下，

$$
f(A,D)=\frac{H(D)-H(D|A)}{H(A)}
$$

> 其中$H(D)$表示变量$D$的信息熵，$H(D)=-\sum_{i=1}^{d}p_i \log p_i$，$d$表示变量$D$中类别的个数，$H(D|A)$表示基于特征$A$的条件熵，$H(D|A)=\sum_{i=1}^{a}{\frac{|D_i|}{|D|}H(D_i)}$,$a$表示变量$A$中特征的类别个数，$H(A)$表示特征$A$的熵，$H(A)=-\sum_{i=1}^{a}\frac{|D_i|}{|D|}\log \frac{|D_i|}{|D|}$，这里需要注意$H(A)$在教材上没有写作$H(A)$的，因为$A$是特征，而是理解成拿特征$A$的取值来划分标签$D$，本质上来说就是$A$的熵。

在函数$f(A,D)$确定后，只需遍历所有特征，找到最大的函数值（信息增益比）所对应的特征即可。

可以看到信息增益比在ID3的信息增益上做了改进，除以了$H(A)$，主要是考虑到信息增益偏向于选择取值数量较多的特征，而信息增益则对此多了放缩，在特征取值较多时，$H(A)$的取值会偏大，从而达到放缩的效果。

**需要注意的是，信息增益对取值较多的特征有所偏好，而信息增益比则对取值较少的特征有所偏好，所以后期，C4.5算法采用了一个启发式策略，先从候选特征中找出信息增益高于平均水平的特征，然后再选择信息增益比最高的，相当于同时考量了信息增益和信息增益比。**

C4.5算法中，添加了对连续类型特征和缺失值的处理，

- 连续类型特征

  对于连续类型特征，采用**二分策略**，即遍历特征的取值，将特征划分为两个类别，然后求信息增益比，然后找到最优的切分点，以此作为该特征的切分点。

- 含缺失值的特征

  首先对该特征中未缺失的部分计算其信息增益比，然后将该特征的**未缺失率**乘以信息增益比，以此作为该特征最终的信息增益比。

### 3.2 决策树的生成

- 类别型特征

  对于类别类型特征，其树的生长方式，类似ID3算法，即特征存在几个取值则分几个支

- 连续类型特征

  上面提到，对于连续类型特征采用二分策略，会遍历特征的取值（相当于将特征划分为两个类别），所以**连续型特征特征选择和分割点这一步是同步进行的**，其树的生长，会将结点分为两个支（按照特征分割点）。

然后迭代第一步和第二步即可以完成整个决策树的创建。

### 3.3 决策树的剪枝

这部分可参考上面**1.3 决策树的修剪**即可

## 4. CART算法

CART(classification and regression tree，分类与回归树)模型是由Breiman等人在1984年提出，是应用最广泛的决策树学习算法。从名字可以看出，CART算法可以处理分类和回归任务（ID3和C4.5算法只能处理分类任务），数结构简化为二叉树。

### 4.1 分类树

分类树主要解决分类任务。

#### 4.1.1 特征选择

在特征选择中，使用**基尼增益**来选择特征，假定特征为$A$，标签为$D$，则特征$A$对$D$的预测能力$f(A,D)$定义如下，
$$
f(A,D)=\sum_{i=1}^{a}\frac{|D_i|}{|D|}Gini(D_i)
$$

> 其中$a$表示特征$A$取值类别的个数，$Gini(D_i)$表示变量$D_i$的基尼值，$Gini(D_i)=\sum_{i=1}^{d_i}p_i(1-p_i)$，$d_i$表示变量$D_i$中类别的个数

在函数$f(A,D)$确定后，只需遍历所有特征，找到最大函数值（基尼增益）所对应的特征即可。

- 连续型特征分割点

  类似C4.5特征的选择，相当于特征和分割点同时确定

- 类别型特征分割点

  由于是二叉树，要求必须是二分，如果特征的取值数量较多，要穷举所有可能比较耗时。

  > 有关类别型特征具体策略不是很清楚，书上没有给出解释，在sklearn中cart tree实现中，不支持类别型的特征，也不支持包含缺失值的特征。

#### 4.1.2 决策树的生成

对于连续类型特征，策略类似C4.5，采用二分策略；对于类别类型特征，由于CART是二叉树，所以需要对类别型特征划分为两个簇。

#### 4.1.3 决策树的剪枝

这部分同样参考**1.3 决策树的修剪**

### 4.2 回归树

回归树主要解决回归任务。

#### 4.2.1 特征选择

因为此时学习目标为连续型的值，所以不能采用信息增益或者基尼增益这种方式。在特征选择中，使用了**均方误差**来选择特征，假定特征为$A$，标签为$D$，则特征$A$对$D$的预测能力$f(A,D)$定义如下，
$$
f(A,D)=\sum_D(y_i-\bar{y})^2 - (\sum_{i=1}^{l}{(y_i-\bar{y_l})^2} + \sum_{j=1}^{r}{(y_j-\bar{y_r})^2})
$$

> 这两部分样本是通过特征$A$的取值进行划分，$l$代表左侧划分样本的数量，$r$代表右侧划分样本的数量，$\bar{y_l}$代表左侧样本标签的均值，$\bar{y_r}$代表右侧样本标签的均值，$\bar{y}$表示划分前所有样本的均值。

在函数$f(A,D)$确定后，只需遍历所有特征，找到最小函数值（均方误差）所对应的特征即可。

后续特征分割点选取和树的剪枝和分类树完全一致，可参考**4.1 分类树**。

## 5. 三种决策树算法的比较

| 算法         | ID3      | C4.5       | CART                        |
| ------------ | -------- | ---------- | --------------------------- |
| 解决的任务   | 分类     | 分类       | 分类&回归                   |
| 决策树类型   | 多叉树   | 多叉树     | 二叉树                      |
| 类别型特征   | 支持     | 支持       | 不支持(sklearn接口中不支持) |
| 连续型特征   | 不支持   | 支持       | 支持                        |
| 含缺失特征   | 不支持   | 支持       | 不支持                      |
| 分类特征选择 | 信息增益 | 信息增益比 | 基尼增益                    |
| 连续特征选择 | 无       | 无         | 均方误差                    |
| 是否存在剪枝 | 不存在   | 存在       | 存在                        |

